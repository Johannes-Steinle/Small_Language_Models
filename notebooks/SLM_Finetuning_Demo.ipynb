{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Johannes-Steinle/Small_Language_Models/blob/main/notebooks/SLM_Finetuning_Demo.ipynb)\n\n# SLM Fine-Tuning & Inferenz Demo: Gemma 3 (QLoRA)\n\nDieses Notebook zeigt den kompletten Workflow eines Small Language Models (SLM):\n1. **Modell laden** — Gemma-3-4B in 4-Bit-Quantisierung (~3–4 GB VRAM)\n2. **VORHER-Test** — Antwort des Basismodells auf eine Fachfrage\n3. **Fine-Tuning** — Anpassung mittels QLoRA auf einem deutschen Instruktions-Datensatz\n4. **NACHHER-Test** — Dieselbe Frage erneut stellen und die Antwort vergleichen\n\n**Voraussetzungen:**\n1. Hugging Face Account\n2. Akzeptierte Gemma-3-Lizenzbedingungen auf Hugging Face\n3. HF Access Token (Read)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Abhängigkeiten installieren\n",
    "!pip install -q -U bitsandbytes transformers peft accelerate trl datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Bei Hugging Face anmelden\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. Modell laden (4-Bit-Quantisierung)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Gemma-3-4B in 4-Bit benötigt ~3–4 GB VRAM — passt auf Colab T4.\nmodel_id = \"google/gemma-3-4b-it\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = \"right\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n\nprint(f\"VRAM-Verbrauch: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n\n# Hilfsfunktion für Inferenz (funktioniert vor und nach dem Training)\ndef generate(prompt, max_tokens=512):\n    model.eval()\n    text = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    input_len = inputs[\"input_ids\"].shape[-1]\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. VORHER-Test — Antwort des Basismodells\ntest_prompt = \"Was ist der Unterschied zwischen LoRA und vollem Fine-Tuning?\"\nprint(f\"Prompt: {test_prompt}\\n\")\nbefore_text = generate(test_prompt)\nprint(\"VORHER-Antwort (Basismodell):\")\nprint(before_text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5. Datensatz laden und tokenisieren\nfrom datasets import load_dataset\n\n# Deutscher Instruktions-Datensatz: 3.721 Frage-Antwort-Paare aus OpenAssistant\ndata = load_dataset(\"mayflowergmbh/oasst_de\", split=\"train\")\nprint(f\"Datensatz: {len(data)} Einträge\")\n\n# Tokenisierung mit Chat-Template und token_type_ids.\n# Gemma 3 benötigt token_type_ids (Multimodal-Architektur: Text vs. Bild).\n# Für reines Text-Training: token_type_ids = 0 (alles Text-Token).\ndef tokenize(example):\n    user_msg = example[\"instruction\"]\n    if example.get(\"input\"):\n        user_msg += \"\\n\" + example[\"input\"]\n    messages = [\n        {\"role\": \"user\", \"content\": user_msg},\n        {\"role\": \"assistant\", \"content\": example[\"output\"]}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n    tokens = tokenizer(text, truncation=True, max_length=512)\n    tokens[\"token_type_ids\"] = [0] * len(tokens[\"input_ids\"])\n    return tokens\n\ndata = data.map(tokenize, remove_columns=data.column_names)\nprint(f\"Tokenisiert: {len(data)} Einträge\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 6. Training (QLoRA Fine-Tuning)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer, SFTConfig\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Data-Collator: Padding + Labels für das Training\ndef gemma3_collator(features):\n    batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n    batch[\"labels\"] = batch[\"input_ids\"].clone()\n    if \"token_type_ids\" not in batch:\n        batch[\"token_type_ids\"] = torch.zeros_like(batch[\"input_ids\"])\n    return batch\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=data,\n    data_collator=gemma3_collator,\n    args=SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=30,  # Für Demo-Zwecke. Für echtes Training deutlich erhöhen.\n        learning_rate=2e-4,\n        bf16=True,\n        logging_steps=10,\n        output_dir=\"outputs\",\n    ),\n    peft_config=lora_config,\n)\n\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "source": "## Ergebnis des Trainings\n\n**Woran erkennt man, dass das Training funktioniert hat?**\n\nDer wichtigste Indikator ist der **Training-Loss**: Er misst, wie gut das Modell die Trainingsdaten vorhersagen kann. Ein sinkender Loss bedeutet, dass das Modell die Muster im Datensatz lernt.\n\n- **Typischer Verlauf:** Der Loss startet bei ~2.5–3.0 und sinkt über die 30 Steps auf ~1.6–1.8.\n- **Was das bedeutet:** Das Modell hat gelernt, deutsche Instruktions-Antworten im Stil des Datensatzes (`oasst_de`) besser vorherzusagen.\n\n**Warum sehen VORHER und NACHHER trotzdem ähnlich aus?**\n\nGemma 3 4B ist bereits ein leistungsfähiges Modell, das Deutsch gut beherrscht. Mit nur 30 Training-Steps (eine Demo-Einstellung) sind die Änderungen subtil — etwa leichte Unterschiede in Wortwahl, Satzstruktur oder Antwortaufbau. Für deutlich sichtbare Unterschiede bräuchte man:\n- **Mehr Training-Steps** (300–1.000+)\n- **Einen spezialisierten Datensatz** (z.B. medizinische Fachtexte, juristischer Stil)\n\nDas Notebook demonstriert den **technischen Workflow** von QLoRA — derselbe Prozess skaliert auf produktionsreife Ergebnisse, wenn man Datensatz und Trainingszeit anpasst.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 7. NACHHER-Test — Antwort nach dem Fine-Tuning\nafter_text = generate(test_prompt)\n\nprint(\"=\" * 60)\nprint(\"VERGLEICH: VORHER vs. NACHHER\")\nprint(\"=\" * 60)\nprint(f\"\\nPrompt: {test_prompt}\")\nprint(f\"\\n--- VORHER (Basismodell) ---\\n{before_text}\")\nprint(f\"\\n--- NACHHER (Fine-Tuned) ---\\n{after_text}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 8. (Optional) Adapter speichern\n# trainer.save_model(\"my_slm_adapter\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}